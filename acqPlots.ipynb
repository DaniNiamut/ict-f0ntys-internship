{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.models.transforms import Normalize, Standardize\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.acquisition import LogExpectedImprovement, UpperConfidenceBound, ProbabilityOfImprovement\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.optim import optimize_acqf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def poly(x):\n",
    "    return (-0.001 * x**6 + 0.003 * x**5 + 0.062 * x**4 - \n",
    "            0.174 * x**3 - 0.673 * x**2 + 1.323 * x + 1.764)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acq_func(train_x, train_y, acq_func, model, true_y = None):\n",
    "\n",
    "    # Plot preperations\n",
    "    plot_x = np.linspace(-5,5,100)\n",
    "    plot_x_tensor = torch.tensor(plot_x.reshape(-1,1))\n",
    "    plot_x_tensor = torch.tensor(plot_x.reshape(-1,1))\n",
    "\n",
    "    # Find the mean\n",
    "    with torch.no_grad():\n",
    "        model_mean = model.posterior(plot_x_tensor).mean, model.posterior(plot_x_tensor).variance\n",
    "\n",
    "    acq_dict={\n",
    "        'LogEI': LogExpectedImprovement(model=model, best_f=train_y.max()),\n",
    "        'UCB': ProbabilityOfImprovement(model=model, best_f=train_y.max()),\n",
    "        'LogPI':ProbabilityOfImprovement(model=model, best_f=train_y.max())\n",
    "    }\n",
    "    \n",
    "    acq_trained = acq_dict[acq_func](model=model, best_f=train_y.max())\n",
    "\n",
    "    bounds = torch.stack([torch.zeros(1), torch.ones(1)]).to(torch.double)\n",
    "    candidate = optimize_acqf(acq_trained, bounds=bounds, q=1, num_restarts=5, raw_samples=20)\n",
    "\n",
    "    fig, axs = plt.subplots(2)\n",
    "    fig.set_size_inches(7, 7)\n",
    "    #axs[0].plot(plot_x,plot_y)\n",
    "    axs[0].scatter(train_x,train_y, color = 'orange')\n",
    "    axs[0].plot(plot_x_tensor.numpy(), model_mean.numpy(), label=\"GP Mean\", color=\"red\", linestyle='dashed')\n",
    "    axs[0].scatter(candidate, poly(candidate), color = 'green', marker='o', s= 50)\n",
    "\n",
    "    #ax2.set_ylim(-20,10)\n",
    "    axs[1].plot(plot_x_tensor.numpy(), acq_trained.numpy(), color=\"orange\", linestyle='dashed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_31704\\1202284431.py:20: InputDataWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  gp = SingleTaskGP(\n",
      "c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\botorch\\acquisition\\multi_objective\\monte_carlo.py:111: NumericsWarning: qExpectedHypervolumeImprovement has known numerical issues that lead to suboptimal optimization performance. It is strongly recommended to simply replace\n",
      "\n",
      "\t qExpectedHypervolumeImprovement \t --> \t qLogExpectedHypervolumeImprovement \n",
      "\n",
      "instead, which fixes the issues and has the same API. See https://arxiv.org/abs/2310.20708 for details.\n",
      "  legacy_ei_numerics_warning(legacy_name=type(self).__name__)\n",
      "c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.acquisition.multi_objective.monte_carlo import qExpectedHypervolumeImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.models.transforms import Normalize, Standardize\n",
    "from botorch.utils.multi_objective.box_decompositions.non_dominated import NondominatedPartitioning\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "\n",
    "# Step 1: Generate 2D synthetic training data\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "train_X = torch.rand(10, 2) * 10 - 5  # 10 points in [-5,5] x [-5,5]\n",
    "train_Y1 = torch.sin(train_X[:, 0]) + torch.cos(train_X[:, 1]) + 0.1 * torch.randn(10)\n",
    "train_Y2 = torch.cos(train_X[:, 0]) - torch.sin(train_X[:, 1]) + 0.1 * torch.randn(10)\n",
    "train_Y = torch.stack([train_Y1, train_Y2], dim=-1)  # Multi-objective outputs\n",
    "\n",
    "# Step 2: Define GP Model\n",
    "gp = SingleTaskGP(\n",
    "    train_X,\n",
    "    train_Y,\n",
    "    input_transform=Normalize(d=2),\n",
    "    outcome_transform=Standardize(m=2),\n",
    ")\n",
    "gp.eval()\n",
    "\n",
    "# Step 3: Define Reference Point (for Hypervolume Calculation)\n",
    "ref_point = train_Y.min(dim=0).values - 0.1  # Slightly below the minimum observed\n",
    "\n",
    "# Step 4: Define EHVI Acquisition Function\n",
    "partitioning = NondominatedPartitioning(ref_point=ref_point, Y=train_Y)\n",
    "ehvi = qExpectedHypervolumeImprovement(model=gp, ref_point=ref_point.tolist(), partitioning=partitioning)\n",
    "\n",
    "# Step 5: Generate a 2D grid for plotting\n",
    "X1 = torch.linspace(-5, 5, 30)\n",
    "X2 = torch.linspace(-5, 5, 30)\n",
    "X1_mesh, X2_mesh = torch.meshgrid(X1, X2, indexing=\"ij\")\n",
    "X_grid = torch.cat([X1_mesh.reshape(-1, 1), X2_mesh.reshape(-1, 1)], dim=1)\n",
    "\n",
    "# Step 6: Compute EHVI over the grid\n",
    "with torch.no_grad():\n",
    "    EHVI_values = ehvi(X_grid).reshape(30, 30)  # Reshape for 3D plotting\n",
    "\n",
    "# Step 7: 3D Plot of EHVI\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot_surface(X1_mesh.numpy(), X2_mesh.numpy(), EHVI_values.numpy(), cmap=\"viridis\", alpha=0.8)\n",
    "ax.set_xlabel(\"X1\")\n",
    "ax.set_ylabel(\"X2\")\n",
    "ax.set_zlabel(\"EHVI\")\n",
    "ax.set_title(\"Expected Hypervolume Improvement (EHVI)\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_31704\\1025535847.py:10: InputDataWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  gp = SingleTaskGP(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected m1 and m2 to have the same dtype, but got: double != float",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[109]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m     gp_mean = gp.posterior(plot_x_tensor).mean  \u001b[38;5;66;03m# Now both are float32\u001b[39;00m\n\u001b[32m     25\u001b[39m     gp_var = gp.posterior(plot_x_tensor).variance\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mplot_acq_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mLogEI\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgp\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[104]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mplot_acq_func\u001b[39m\u001b[34m(train_x, train_y, acq_func, model, true_y)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Find the mean\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     model_mean = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mposterior\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplot_x_tensor\u001b[49m\u001b[43m)\u001b[49m.mean, model.posterior(plot_x_tensor).variance\n\u001b[32m     12\u001b[39m acq_dict={\n\u001b[32m     13\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mLogEI\u001b[39m\u001b[33m'\u001b[39m: LogExpectedImprovement(model=model, best_f=train_y.max()),\n\u001b[32m     14\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mUCB\u001b[39m\u001b[33m'\u001b[39m: ProbabilityOfImprovement(model=model, best_f=train_y.max()),\n\u001b[32m     15\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mLogPI\u001b[39m\u001b[33m'\u001b[39m:ProbabilityOfImprovement(model=model, best_f=train_y.max())\n\u001b[32m     16\u001b[39m }\n\u001b[32m     19\u001b[39m acq_trained = acq_dict[acq_func](model=model, best_f=train_y.max())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\botorch\\models\\gpytorch.py:448\u001b[39m, in \u001b[36mBatchedMultiOutputGPyTorchModel.posterior\u001b[39m\u001b[34m(self, X, output_indices, observation_noise, posterior_transform)\u001b[39m\n\u001b[32m    442\u001b[39m     X, output_dim_idx = add_output_dim(\n\u001b[32m    443\u001b[39m         X=X, original_batch_shape=\u001b[38;5;28mself\u001b[39m._input_batch_shape\n\u001b[32m    444\u001b[39m     )\n\u001b[32m    445\u001b[39m \u001b[38;5;66;03m# NOTE: BoTorch's GPyTorchModels also inherit from GPyTorch's ExactGP, thus\u001b[39;00m\n\u001b[32m    446\u001b[39m \u001b[38;5;66;03m# self(X) calls GPyTorch's ExactGP's __call__, which computes the posterior,\u001b[39;00m\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# rather than e.g. SingleTaskGP's forward, which computes the prior.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m mvn = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    449\u001b[39m mvn = \u001b[38;5;28mself\u001b[39m._apply_noise(X=X, mvn=mvn, observation_noise=observation_noise)\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_outputs > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gpytorch\\models\\exact_gp.py:345\u001b[39m, in \u001b[36mExactGP.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;66;03m# Make the prediction\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m settings.cg_tolerance(settings.eval_cg_tolerance.value()):\n\u001b[32m    342\u001b[39m     (\n\u001b[32m    343\u001b[39m         predictive_mean,\n\u001b[32m    344\u001b[39m         predictive_covar,\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprediction_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexact_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_covar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;66;03m# Reshape predictive mean to match the appropriate event shape\u001b[39;00m\n\u001b[32m    348\u001b[39m predictive_mean = predictive_mean.view(*batch_shape, *test_shape).contiguous()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gpytorch\\models\\exact_prediction_strategies.py:324\u001b[39m, in \u001b[36mDefaultPredictionStrategy.exact_prediction\u001b[39m\u001b[34m(self, joint_mean, joint_covar)\u001b[39m\n\u001b[32m    320\u001b[39m     test_test_covar = joint_covar[..., \u001b[38;5;28mself\u001b[39m.num_train :, \u001b[38;5;28mself\u001b[39m.num_train :]\n\u001b[32m    321\u001b[39m     test_train_covar = joint_covar[..., \u001b[38;5;28mself\u001b[39m.num_train :, : \u001b[38;5;28mself\u001b[39m.num_train]\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexact_predictive_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_train_covar\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    325\u001b[39m     \u001b[38;5;28mself\u001b[39m.exact_predictive_covar(test_test_covar, test_train_covar),\n\u001b[32m    326\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gpytorch\\models\\exact_prediction_strategies.py:349\u001b[39m, in \u001b[36mDefaultPredictionStrategy.exact_predictive_mean\u001b[39m\u001b[34m(self, test_mean, test_train_covar)\u001b[39m\n\u001b[32m    347\u001b[39m nan_policy = settings.observation_nan_policy.value()\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nan_policy == \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m     res = (\u001b[43mtest_train_covar\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m).squeeze(-\u001b[32m1\u001b[39m)\n\u001b[32m    350\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nan_policy == \u001b[33m\"\u001b[39m\u001b[33mmask\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    351\u001b[39m     \u001b[38;5;66;03m# Restrict train dimension to observed values\u001b[39;00m\n\u001b[32m    352\u001b[39m     observed = settings.observation_nan_policy._get_observed(mean_cache, torch.Size((mean_cache.shape[-\u001b[32m1\u001b[39m],)))\n",
      "\u001b[31mRuntimeError\u001b[39m: expected m1 and m2 to have the same dtype, but got: double != float"
     ]
    }
   ],
   "source": [
    "# Generate training data\n",
    "train_x = np.random.uniform(low=-5, high=5, size=10).astype(np.float32)  # Convert to float32\n",
    "train_y = poly(train_x).astype(np.float32)  # Convert to float32\n",
    "\n",
    "# Convert to PyTorch tensors with float32 dtype\n",
    "train_x = torch.tensor(train_x.reshape(-1,1), dtype=torch.float32)\n",
    "train_y = torch.tensor(train_y.reshape(-1,1), dtype=torch.float32)\n",
    "\n",
    "# Define GP model\n",
    "gp = SingleTaskGP(\n",
    "    train_X=train_x,\n",
    "    train_Y=train_y,\n",
    "    input_transform=Normalize(d=1),\n",
    "    outcome_transform=Standardize(m=1),\n",
    ")\n",
    "\n",
    "# Generate test points\n",
    "plot_x = np.linspace(-5, 5, 100).astype(np.float32)  # Convert to float32\n",
    "plot_y = poly(plot_x).astype(np.float32)  # Convert to float32\n",
    "plot_x_tensor = torch.tensor(plot_x.reshape(-1,1), dtype=torch.float32)  # Ensure float32\n",
    "\n",
    "# Compute posterior mean and variance\n",
    "with torch.no_grad():\n",
    "    gp_mean = gp.posterior(plot_x_tensor).mean  # Now both are float32\n",
    "    gp_var = gp.posterior(plot_x_tensor).variance\n",
    "    \n",
    "plot_acq_func(train_x, train_y, 'LogEI', gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.random.uniform(low = -5, high = 5, size = 10)\n",
    "Y = poly(train_X)\n",
    "Y = torch.tensor(Y.reshape(-1,1))# + 0.1 * torch.randn_like(Y)  # add some noise\n",
    "train_X = torch.tensor(train_X.reshape(-1,1))\n",
    "\n",
    "gp = SingleTaskGP(\n",
    "  train_X=train_X,\n",
    "  train_Y=Y,\n",
    "  input_transform=Normalize(d=1),\n",
    "  outcome_transform=Standardize(m=1)\n",
    ")\n",
    "\n",
    "plot_x = np.linspace(-5,5,100)\n",
    "plot_y = poly(plot_x)\n",
    "plot_x_tensor = torch.tensor(plot_x.reshape(-1,1))\n",
    "plot_x_tensor = torch.tensor(plot_x.reshape(-1,1))\n",
    "\n",
    "with torch.no_grad():\n",
    "    gp_mean, gp_var = gp.posterior(plot_x_tensor).mean, gp.posterior(plot_x_tensor).variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.764"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logEI = LogExpectedImprovement(model=gp, best_f=Y.max())\n",
    "EI = logEI(plot_x_tensor.unsqueeze(-2)).detach()\n",
    "\n",
    "bounds = torch.stack([torch.zeros(1), torch.ones(1)]).to(torch.double)\n",
    "candidate, acq_value = optimize_acqf(\n",
    "  logEI, bounds=bounds, q=1, num_restarts=5, raw_samples=20,\n",
    ")\n",
    "num = candidate.item()\n",
    "poly(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
